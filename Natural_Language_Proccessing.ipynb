{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqfkvEmgg5NU"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import imshow, show, subplot, figure, axis, plot, xlabel,ylabel,title,savefig\n",
        " ######################################################################################\n",
        "######################################################################################\n",
        "################################      Question 2       #####################################\n",
        "######################################################################################\n",
        "######################################################################################\n",
        "def q2(): \n",
        "    filename = \"data2.h5\"\n",
        "    with h5py.File(filename, \"r\") as f:\n",
        "        # List all groups\n",
        "        print(\"Keys: %s\" % f.keys())\n",
        "        testSet_Y = f[list(f.keys())[0]][:]\n",
        "        testSet_X = f[list(f.keys())[1]][:]\n",
        "        trainSet_Y = f[list(f.keys())[2]][:]\n",
        "        trainSet_X = f[list(f.keys())[3]][:]\n",
        "        valueOf_y = f[list(f.keys())[4]][:]\n",
        "        valueOf_x = f[list(f.keys())[5]][:]\n",
        "        words = f[list(f.keys())[6]][:]\n",
        "        \n",
        "    def vectorOfWord(x, maxInd = 250):\n",
        "      \"\"\"\n",
        "      The vectorOfWord function converts a word index to a one-hot encoded vector. \n",
        "      The input x is an integer representing the index of a word in a vocabulary \n",
        "      of words. The one-hot encoded vector is a list of zeros with a single element \n",
        "      of 1 at the index x-1,which represents the position of the word in the\n",
        "      vocabulary. The optional parameter maxInd specifies the size of the \n",
        "      one-hot encoded vector, and it is set to 250 by default.\n",
        "\n",
        "      \"\"\"\n",
        "      out = np.zeros(maxInd)\n",
        "      out[x-1] = 1\n",
        "      return out\n",
        "\n",
        "    def input_vector(data):\n",
        "      \"\"\"\n",
        "      he input_vector function encodes a list of three-word phrases as a \n",
        "      list of one-hot encoded vectors.The input data is a 2D array where \n",
        "      each row represents a three-word phrase, with the first, second, \n",
        "      and third elements representing the indices of the words in the \n",
        "      vocabulary.\n",
        "\n",
        "      \"\"\"\n",
        "      dataEncoded = []\n",
        "      for row in data:\n",
        "          word_1 = vectorOfWord(row[0])\n",
        "          word_2 = vectorOfWord(row[1])\n",
        "          word_3 = vectorOfWord(row[2])\n",
        "          row = np.concatenate((word_1,word_2,word_3))\n",
        "          row = row.reshape(1,len(row))\n",
        "          dataEncoded.append(row)\n",
        "      return dataEncoded\n",
        "\n",
        "    def output_vector(data):\n",
        "      \"\"\"\n",
        "      The function first initializes an empty list dataEncoded. It then iterates\n",
        "      over each element in data, converts the word index to a one-hot encoded \n",
        "      vector using the vectorOfWord function, and adds it to the dataEncoded list. \n",
        "      Finally, the function returns the dataEncoded list.\n",
        "      \"\"\"\n",
        "      dataEncoded = []\n",
        "      for row in data:\n",
        "          word = vectorOfWord(row)\n",
        "          dataEncoded.append(word)\n",
        "      return dataEncoded\n",
        "\n",
        "    def init_coeff(D,P,data,mean=0,std=0.01):\n",
        "      \"\"\"\n",
        "       To initialize the weights and biases of a neural network with a particular\n",
        "       architecture. The weights and biases are initialized as random normal \n",
        "       variables with a given mean and standard deviation.\n",
        "      \"\"\"\n",
        "      N = 200\n",
        "      W0 = np.random.normal(mean,std, 750*D).reshape(750, D)\n",
        "      W1 = np.random.normal(mean,std, D*P).reshape(D,P)\n",
        "      W2 = np.random.normal(mean,std, P*250).reshape(P,250)\n",
        "      b1 = np.random.normal(mean,std, N*P).reshape(N,P)\n",
        "      b2 = np.random.normal(mean,std, N*250).reshape(N,250)\n",
        "      \n",
        "      Mul = [W0,W1,W2,b1,b2]\n",
        "      \n",
        "      return Mul\n",
        "\n",
        "    def sigmoid(x):\n",
        "        y = 1 / (1 + np.exp(-x))\n",
        "        return y\n",
        "\n",
        "    def sigmoid_backward(x):\n",
        "      \"\"\"\n",
        "      This function can be used in the backpropagation algorithm to calculate \n",
        "      the gradient of the loss function with respect to the weights and biases \n",
        "      of the neural network. Backpropagation is an algorithm used to train \n",
        "      neural networks by updating the weights and biases to minimize the loss \n",
        "      function. The gradients calculated using backpropagation are used to update \n",
        "      the weights and biases using an optimization algorithm such as stochastic\n",
        "      gradient descent.\n",
        "      \"\"\"\n",
        "      d_sig = x*(1-x)\n",
        "      return d_sig\n",
        "\n",
        "    def softmax(x):\n",
        "      \"\"\"\n",
        "      Softmax function, which is a generalization of the logistic function to \n",
        "      multiple classes. It is often used as the activation function for the \n",
        "      output layer of a neural network when the network is used for classification \n",
        "      tasks with more than two classes.\n",
        "      \"\"\"\n",
        "      expx = np.exp(x - np.max(x))\n",
        "      y = expx/np.sum(expx, axis=0)\n",
        "      return y\n",
        "\n",
        "    def cross_entrophy(y,y_pred):\n",
        "      \"\"\"\n",
        "      The cross-entropy loss function, which is a commonly used loss function \n",
        "      for classification tasks. \n",
        "      \"\"\"\n",
        "      return (np.sum(- y * np.log(y_pred))/ y.shape[0])\n",
        "\n",
        "    def forward(data, Mul):\n",
        "      \"\"\"\n",
        "      The function you provided appears to implement the forward pass of a neural \n",
        "      network with a particular architecture. The forward pass is the process of\n",
        "      computing the output of the neural network given an input data point.\n",
        "\n",
        "      The function takes in two arguments: data and Mul. The variable data \n",
        "      represents the input data and has shape (batch size, number of features). \n",
        "      The variable Mul is a list containing the weights and biases of the neural \n",
        "      network. The function unpacks Mul into the variables w0, w1, w2, b1, and b2, \n",
        "      which represent the weights and biases of the first, second, and output \n",
        "      layers of the neural network, respectively.\n",
        "      \"\"\" \n",
        "      \n",
        "      w0,w1,w2,b1,b2 = Mul\n",
        "\n",
        "      z0 = np.dot(data,w0) #first layer linear forward\n",
        "      A0 = z0 #first layer without activation\n",
        "      z1 = np.dot(A0,w1) + b1 #second layer linear forward\n",
        "      A1 = sigmoid(z1) #second layer activation\n",
        "      z2 =  np.dot(A1,w2) + b2 #output linear forward\n",
        "      output = softmax(z2) #output layer activation\n",
        "\n",
        "      return A0,A1,output\n",
        "\n",
        "    def calculationOfCost(data,y,Mul):\n",
        "      \"\"\"\n",
        "      The function takes in three arguments: data, y, and Mul. The variable data \n",
        "      represents the input data and has shape (batch size, number of features). \n",
        "      The variable y represents the true labels and has shape (batch size, number \n",
        "      of classes). The variable Mul is a list containing the weights and biases \n",
        "      of the neural network.\n",
        "\n",
        "    \"\"\"\n",
        "      N = data.shape[0]\n",
        "      W0,W1,W2,b1,b2 = Mul\n",
        "      A0,A1,output = forward(data, Mul)\n",
        "      d_sig = sigmoid_backward(A1)\n",
        "  \n",
        "      amountOfCost = cross_entrophy(y,output)\n",
        "\n",
        "      d_w0 = np.dot(data.T,((np.dot(((np.dot((y-output),W2.T))*d_sig),W1.T))*A0))\n",
        "      d_w1 = np.dot(A0.T,(np.dot((y-output),W2.T)*d_sig))\n",
        "      d_w2 = np.dot(A1.T,(y-output))\n",
        "      d_b1 = np.dot((y-output),W2.T)*d_sig\n",
        "      d_b2 = y-output\n",
        "      \n",
        "      grads = [d_w0,d_w1,d_w2,d_b1,d_b2]\n",
        "      \n",
        "      return amountOfCost, grads\n",
        "\n",
        "    def backwardPropagation(data,y,lr_rate,momentum, Mul, old_grads):\n",
        "      \"\"\"\n",
        "      The function first calculates the cost and gradients of the neural network\n",
        "      using the forward and backward propagation algorithms. It then updates the \n",
        "      weights and biases of the network using the gradients and the learning rate\n",
        "      and momentum hyperparameters. The updated weights and biases are stored in Mul.\n",
        "      Finally, the function returns the cost (amountOfCost), grads, and Mul \n",
        "      as a tuple.\n",
        "      \"\"\"\n",
        "      \n",
        "      amountOfCost, grads = calculationOfCost(data,y,Mul)\n",
        "  \n",
        "      Mul[0] -= lr_rate*grads[0]+old_grads[0]*momentum\n",
        "      Mul[1] -= lr_rate*grads[1]+old_grads[1]*momentum\n",
        "      Mul[2] -= lr_rate*grads[2]+old_grads[2]*momentum\n",
        "      Mul[3] -= lr_rate*grads[3]+old_grads[3]*momentum\n",
        "      Mul[4] -= lr_rate*grads[4]+old_grads[4]*momentum\n",
        "\n",
        "      return amountOfCost, grads, Mul\n",
        "        \n",
        "    def train(trainSet_X,trainSet_Y,D,P,epoch,num_batch,lr_rate,momentum):\n",
        "      \"\"\"\n",
        "      To iterate over a number of epochs, and within each epoch it divides the\n",
        "      training data into batches and performs the training by calling the \n",
        "      backwardPropagation function with the current batch of data and the \n",
        "      current model coefficients. The function also appears to keep track of \n",
        "      the cost (or loss) at each epoch and return the final model coefficients,\n",
        "      the list of costs, and the list of epochs.\n",
        "      \"\"\"\n",
        "      amountOfCosts = []\n",
        "      epochs = []\n",
        "      data = input_vector(trainSet_X)\n",
        "      data = np.squeeze(data,axis=1)\n",
        "      label = output_vector(trainSet_Y)\n",
        "      label = np.array(label)\n",
        "      Mul = init_coeff(D,P,data,mean=0,std=0.01)\n",
        "      momentum = 0.0000085\n",
        "      amountOfCost, old_grads = calculationOfCost(data[:200],label[0:200],Mul)\n",
        "      lr_rate = 0.0000015\n",
        "      for i in range (epoch):\n",
        "          epochs.append(i)\n",
        "          for j in range (num_batch):\n",
        "              data_batch = data[200*j:200*j+200]\n",
        "              label_batch = label[200*j:200*j+200]\n",
        "              amountOfCost, grads,Mul = backwardPropagation(data_batch,label_batch, lr_rate,momentum,Mul,old_grads)\n",
        "          amountOfCosts.append(amountOfCost)\n",
        "      j = 0\n",
        "      for i in reversed (amountOfCosts):\n",
        "          print(\"Epoch: {} --------------> Loss: {} \".format(j+1,i))\n",
        "          j+=1\n",
        "      return Mul,amountOfCosts[::-1],epochs\n",
        "\n",
        "    def random_index(sample_size):\n",
        "       \"\"\"\n",
        "       The function takes in a single input, sample_size, which specifies the \n",
        "       length of the list of random indices to generate. The function uses a \n",
        "       for loop to generate sample_size number of random indices using the \n",
        "       random.randint function and appends each one to a list. Finally, the \n",
        "       function returns the list of random indices.\n",
        "\n",
        "       \"\"\"\n",
        "       random_index = []\n",
        "       for i in range(sample_size):\n",
        "          index = random.randint(0,46500)\n",
        "          random_index.append(index)\n",
        "       return random_index\n",
        "\n",
        "    def choose_Sample(data,label,sample_size):\n",
        "      \"\"\"\n",
        "      This is a function that selects a random sample of data from a larger dataset. \n",
        "      It looks like the function takes in three inputs:\n",
        "\n",
        "      data, which is the larger dataset from which the sample will be drawn\n",
        "      \n",
        "      label, which appears to be a list of labels corresponding to the data points in data\n",
        "      \n",
        "      sample_size, which specifies the number of data points to include in the sample\n",
        "      \n",
        "      The function calls the random_index function to generate a list of sample_size \n",
        "      number of random indices, and then uses a for loop to select the corresponding \n",
        "      data points and labels from data and label using the generated indices. \n",
        "      Finally, the function returns the selected sample of data and \n",
        "      the corresponding labels.\n",
        "      \"\"\"\n",
        "      sample = []\n",
        "      labels = []\n",
        "      sample_index = random_index(sample_size)\n",
        "      for i in sample_index:\n",
        "          sample.append(data[i])\n",
        "          labels.append(label[i])\n",
        "      return sample,labels\n",
        "\n",
        "    def predict(words,output):\n",
        "      \"\"\"\n",
        "      To be making predictions based on a list of output vectors and a list of \n",
        "      words.The function takes in two inputs:\n",
        "\n",
        "      words, which is a list of words\n",
        "      \n",
        "      output, which is a list of output vectors\n",
        "      \n",
        "      The function uses a for loop to iterate over the list of output vectors\n",
        "      and for each vector, it uses the argsort method to get the indices of the\n",
        "      k largest values, where k is 10 in this case. The function then uses a \n",
        "      second for loop to iterate over the list of indices and selects the \n",
        "      corresponding words from the words list. The function appends the list \n",
        "      of selected words to a list called pred_rows and returns this list at the end.\n",
        "\n",
        "      \"\"\"\n",
        "      pred_rows = []\n",
        "      for i in range(len(output)):\n",
        "          word_index = output[i].argsort()[-10:][::-1]\n",
        "          pred_words = []\n",
        "          for word in word_index: \n",
        "              pred_words.append(str(words[word].decode(\"utf-8\")))\n",
        "          pred_rows.append((pred_words))\n",
        "      return pred_rows\n",
        "\n",
        "    def print_preds(casual_example,test_label,pred_rows,words):\n",
        "      \"\"\"\n",
        "      This functions prints the predictions that we get from the earlier functions\n",
        "      \"\"\"\n",
        "      for i in range(len(casual_example)):\n",
        "          tri = \"sample trigram: \"\n",
        "          for j in range(len(casual_example[i])):\n",
        "              tri+=str(words[casual_example[i][j]].decode(\"utf-8\"))+\" \"\n",
        "          tri += \" ----> label: \" + str(words[test_label[i]].decode(\"utf-8\"))\n",
        "          print(tri)\n",
        "          print(\"Top 10 predictions: \",pred_rows[i])\n",
        "          \n",
        "    def plot_losses(losses,epochs,titles):\n",
        "      \"\"\"\n",
        "      The function using matplotlib to create the plot. It sets the x-axis label\n",
        "      to \"epoch\", the y-axis label to \"loss\", and the title to the input titles.\n",
        "      It then uses the plot function to plot the losses versus the epochs.\n",
        "\n",
        "      \"\"\"\n",
        "      xlabel(\"epoch\")\n",
        "      ylabel(\"loss\")\n",
        "      title(titles)\n",
        "      plot(epochs,losses)\n",
        "    ########################## Test with different D and P values ######################\n",
        "    \n",
        "    \"\"\"epoch changed to 5 in order to save tim TA while running main idea is to show code is running\n",
        "    (default is 30 if you want to try you can change by yourself 5th parameter)\"\"\"\n",
        "    \n",
        "    Mul,amountOfCosts,epochs = train(trainSet_X,trainSet_Y,32,256,5,1000,0.15,0.85)\n",
        "    \"\"\"Mul,amountOfCosts,epochs = train(valueOf_x,valueOf_y,32,256,30,232,0.15,0.85)\"\"\"\n",
        "    plot_losses(amountOfCosts,epochs,\"D=32 - P=256\")\n",
        "    \n",
        "    \"\"\"this part is commented in order to save time for TA to while running\n",
        "    \n",
        "    Mul,amountOfCosts,epochs = train(trainSet_X,trainSet_Y,16,128,30,1000,0.15,0.85)\n",
        "    Mul,amountOfCosts,epochs = train(valueOf_x,valueOf_y,16,32,30,232,0.15,0.85)\n",
        "    plot_losses(amountOfCosts,epochs,\"D=16 - P=128\")\n",
        "    \n",
        "    Mul,amountOfCosts,epochs = train(trainSet_X,trainSet_Y,8,64,30,1000,0.15,0.85)\n",
        "    Mul,amountOfCosts,epochs = train(valueOf_x,valueOf_y,16,32,30,232,0.15,0.85)\n",
        "    plot_losses(amountOfCosts,epochs,\"D=8 - P=64\")\"\"\"\n",
        "    \n",
        "    ######################## Make Predictions with Test Data ##############################\n",
        "    casual_example,test_label = choose_Sample(testSet_X,testSet_Y,200)\n",
        "    casual_example_vector = input_vector(casual_example)\n",
        "    casual_example_vector = np.squeeze(casual_example_vector,axis=1)\n",
        "    casual_example = casual_example[:5]\n",
        "    test_label  = test_label[:5]\n",
        "    \n",
        "    _,_,output = forward(casual_example_vector, Mul)\n",
        "    output = output[:5]\n",
        "    \n",
        "    pred_rows = predict(words,output)\n",
        "    print_preds(casual_example,test_label,pred_rows,words)\n",
        "\n"
      ]
    }
  ]
}